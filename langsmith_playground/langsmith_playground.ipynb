{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['answer_a', 'answer_b', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You may choose one assistant that follows the user's instructions and answers the user's question better, indicate if both answers are equally good, or indicate if neither answer is satisfactory. Each evaluation should be made independently without comparing to previous evaluations. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['answer_a', 'answer_b', 'question'], input_types={}, partial_variables={}, template=\"[User Question] {question}\\n[The Start of Assistant A's Answer] {answer_a} [The End of Assistant A's Answer]\\nThe Start of Assistant B's Answer] {answer_b} [The End of Assistant B's Answer]\"), additional_kwargs={})])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from typing import Literal\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain import hub\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langsmith import evaluate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "# See the prompt: https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2\n",
        "# prompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "class PreferenceResult(BaseModel):\n",
        "    \"\"\"Result of the preference evaluation between two AI responses\"\"\"\n",
        "\n",
        "    preferred_assistant: Literal[\"A\", \"B\", \"Tie\"] = Field(description=\"Which assistant provided the better response - A, B, or Tie if equal\")\n",
        "    explanation: str = Field(description=\"Detailed explanation of the reasoning behind the preference, analyzing the quality, accuracy, and effectiveness of the responses\")\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You may choose one assistant that follows the user's instructions and answers the user's question better, indicate if both answers are equally good, or indicate if neither answer is satisfactory. Each evaluation should be made independently without comparing to previous evaluations. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.\"),\n",
        "        (\"human\", \"[User Question] {question}\\n[The Start of Assistant A's Answer] {answer_a} [The End of Assistant A's Answer]\\nThe Start of Assistant B's Answer] {answer_b} [The End of Assistant B's Answer]\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "model = init_chat_model(\"gpt-4o\")\n",
        "chain = prompt | model.with_structured_output(PreferenceResult)\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation result: preferred_assistant='A' explanation=\"Assistant A provides a more comprehensive answer by not only stating that Paris is the capital of France but also adding additional context about Paris being a global center for art, fashion, and culture, and mentioning the Eiffel Tower as an iconic landmark. This additional detail makes the response more informative and engaging. On the other hand, Assistant B gives a correct answer but lacks the extra depth and context provided by Assistant A. Therefore, Assistant A's response is preferred.\"\n",
            "Evaluation result 2: preferred_assistant='Tie' explanation='Both Assistant A and Assistant B provided identical responses to the question on how to make a peanut butter sandwich. Each response clearly outlines the steps needed to make the sandwich, including an optional step to cut it diagonally. The instructions are simple, straightforward, and easy to follow, making both responses equally satisfactory in terms of helpfulness, relevance, accuracy, and detail. Therefore, neither response is better than the other, resulting in a tie.'\n"
          ]
        }
      ],
      "source": [
        "# Test the chain with some example responses\n",
        "test_question = \"What is the capital of France?\"\n",
        "response_a = \"The capital of France is Paris, a global center for art, fashion, and culture. It's known for iconic landmarks like the Eiffel Tower.\"\n",
        "response_b = \"Paris is France's capital city.\"\n",
        "\n",
        "result = chain.invoke({\"question\": test_question, \"answer_a\": response_a, \"answer_b\": response_b})\n",
        "print(f\"Evaluation result: {result}\")\n",
        "\n",
        "# Try another example\n",
        "test_question_2 = \"How do you make a peanut butter sandwich?\"\n",
        "response_a_2 = \"To make a peanut butter sandwich: 1. Take two slices of bread 2. Spread peanut butter evenly on one slice 3. Place the second slice on top 4. Optional: cut diagonally for easier eating\"\n",
        "response_b_2 = \"To make a peanut butter sandwich: 1. Take two slices of bread 2. Spread peanut butter evenly on one slice 3. Place the second slice on top 4. Optional: cut diagonally for easier eating\"\n",
        "\n",
        "result_2 = chain.invoke({\"question\": test_question_2, \"answer_a\": response_a_2, \"answer_b\": response_b_2})\n",
        "print(f\"Evaluation result 2: {result_2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ranked_preference(inputs: dict, outputs: list[dict]) -> list:\n",
        "    # Assumes example inputs have a 'question' key and experiment\n",
        "    # outputs have an 'answer' key.\n",
        "    response = chain.invoke(\n",
        "        {\n",
        "            \"question\": inputs[\"question\"],\n",
        "            \"answer_a\": outputs[0].get(\"answer\", \"N/A\"),\n",
        "            \"answer_b\": outputs[1].get(\"answer\", \"N/A\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if response[\"Preference\"] == 1:\n",
        "        scores = [1, 0]\n",
        "    elif response[\"Preference\"] == 2:\n",
        "        scores = [0, 1]\n",
        "    else:\n",
        "        scores = [0, 0]\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate(\n",
        "    (\"experiment-1\", \"experiment-2\"),  # Replace with the names/IDs of your experiments\n",
        "    evaluators=[ranked_preference],\n",
        "    randomize_order=True,\n",
        "    max_concurrency=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langsmith import Client\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "client = Client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For other dataset creation methods, see:\n",
        "# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically\n",
        "# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application\n",
        "\n",
        "# Create inputs and reference outputs\n",
        "examples = [\n",
        "    (\n",
        "        \"Which country is Mount Kilimanjaro located in?\",\n",
        "        \"Mount Kilimanjaro is located in Tanzania.\",\n",
        "    ),\n",
        "    (\n",
        "        \"What is Earth's lowest point?\",\n",
        "        \"Earth's lowest point is The Dead Sea.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "inputs = [{\"question\": input_prompt} for input_prompt, _ in examples]\n",
        "outputs = [{\"answer\": output_answer} for _, output_answer in examples]\n",
        "\n",
        "try:\n",
        "    # Programmatically create a dataset in LangSmith\n",
        "    dataset = client.create_dataset(dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\")\n",
        "\n",
        "    # Add examples to the dataset\n",
        "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n",
        "except Exception as e:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the application logic you want to evaluate inside a target function\n",
        "# The SDK will automatically send the inputs from the dataset to your target function\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "def target(inputs: dict) -> dict:\n",
        "    prompt = ChatPromptTemplate.from_messages([(\"user\", \"{question}\")])\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"question\": inputs[\"question\"]})\n",
        "    return {\"response\": response.content}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define instructions for the LLM judge evaluator\n",
        "instructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: \n",
        "- False: No conceptual match and similarity\n",
        "- True: Most or full conceptual match and similarity\n",
        "- Key criteria: Concept should match, not exact wording.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Define output schema for the LLM judge\n",
        "class Grade(BaseModel):\n",
        "    score: bool = Field(description=\"Boolean that indicates whether the response is accurate relative to the reference answer\")\n",
        "    explanation: str = Field(description=\"Explanation of the grading decision\")\n",
        "\n",
        "\n",
        "# Define LLM judge that grades the accuracy of the response relative to reference output\n",
        "def accuracy(outputs: dict, reference_outputs: dict) -> bool:\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", instructions),\n",
        "            (\"user\", \"Ground Truth answer: {answer}; Student's Answer: {response}\"),\n",
        "        ]\n",
        "    )\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    chain = prompt | llm.with_structured_output(Grade)\n",
        "    response = chain.invoke({\"answer\": reference_outputs[\"answer\"], \"response\": outputs[\"response\"]})\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score=True explanation=\"Both answers describe the same concept: the movement of Earth in relation to the Sun. The term 'orbits' in the ground truth is synonymous with 'revolves' in the student's answer, and both imply the elliptical nature of this movement.\"\n"
          ]
        }
      ],
      "source": [
        "# Test with dummy data\n",
        "test_outputs = {\"response\": \"The Earth revolves around the Sun\"}\n",
        "test_reference = {\"answer\": \"Our planet Earth orbits the Sun in an elliptical path\"}\n",
        "res = accuracy(test_outputs, test_reference)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res.score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'first-eval-in-langsmith-c36b9a4e' at:\n",
            "https://smith.langchain.com/o/eb122562-97bd-51d4-9e3f-86c9acffa2bc/datasets/5089c08c-d8d9-41fe-9474-f412e34bdcec/compare?selectedSessions=852bf841-39cc-4d60-89d9-70b6cff9ef1b\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6d1ea8fc78b488b86671b8f7ab00520",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# After running the evaluation, a link will be provided to view the results in langsmith\n",
        "experiment_results = client.evaluate(\n",
        "    target,\n",
        "    data=\"Sample dataset\",\n",
        "    evaluators=[\n",
        "        accuracy,\n",
        "        # can add multiple evaluators here\n",
        "    ],\n",
        "    experiment_prefix=\"first-eval-in-langsmith\",\n",
        "    max_concurrency=2,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
