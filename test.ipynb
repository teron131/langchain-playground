{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a445676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = \"openai/gpt-5-mini\"\n",
    "temperature = 0.0\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=model,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=temperature,\n",
    "    reasoning_effort=\"medium\",\n",
    "    include_response_headers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a0fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 17:16:47,159 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Explain gradient descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62921ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Short definition\\n- Gradient descent is an iterative optimization algorithm that finds (local) minima of a differentiable function by moving parameters in the direction opposite to the gradient.\\n\\nIntuition\\n- The gradient at a point points in the direction of steepest increase. Moving a small step opposite the gradient decreases the function value most quickly locally. Repeating these steps moves you toward a minimum.\\n\\nMathematics (core update)\\n- Given a loss/objective J(θ) and learning rate η > 0:\\n  θ ← θ − η ∇J(θ)\\n- ∇J(θ) is the vector of partial derivatives (the gradient). The negative sign makes the update go downhill.\\n\\nSimple 1D example\\n- Let f(x) = (x − 3)^2. Then f′(x) = 2(x − 3).\\n- Update: x ← x − η·2(x − 3).\\n- If x0 = 0 and η = 0.1, x1 = 0 + 0.6 = 0.6. Repeating moves x toward 3.\\n\\nVariants\\n- Batch gradient descent: use the full dataset to compute ∇J (stable but expensive for large data).\\n- Stochastic gradient descent (SGD): use the gradient from one training example at a time (noisy, faster per update, useful for large datasets).\\n- Mini-batch gradient descent: use a small batch of examples (common in practice).\\n- Momentum: adds a velocity term to smooth updates and accelerate convergence.\\n- Adaptive methods (AdaGrad, RMSProp, Adam, etc.): adjust per-parameter learning rates based on past gradients.\\n\\nKey hyperparameters and effects\\n- Learning rate (η): too large ⇒ divergence or oscillation; too small ⇒ very slow convergence. Typical practice: tune η, use schedules (decay) or adaptive optimizers.\\n- Batch size: small batches add noise (can help escape shallow minima/saddle points), large batches give stable gradients but higher cost per update.\\n- Momentum and decay parameters: speed convergence and stabilize updates.\\n\\nConvergence and behavior\\n- For convex, smooth functions gradient descent converges to the global minimum (with appropriate step sizes).\\n- For nonconvex problems (e.g., deep neural networks) it converges to a local minimum or saddle point; SGD’s noise often helps find good solutions.\\n- For a quadratic f(x) = 1/2 a x^2, gradient descent converges if 0 < η < 2/a.\\n\\nStopping criteria\\n- Fixed number of iterations/epochs.\\n- Gradient norm small: ||∇J(θ)|| < tolerance.\\n- Change in loss or parameters below threshold.\\n- Validation performance stops improving.\\n\\nPractical tips\\n- Scale/normalize features (helps gradient conditioning).\\n- Initialize parameters sensibly (random small values, or methods like Xavier/He for networks).\\n- Monitor training and validation loss to detect divergence/overfitting.\\n- Use mini-batches and an adaptive optimizer (Adam) as reasonable defaults for many ML problems.\\n- Consider gradient clipping when gradients can explode (RNNs).\\n\\nPseudocode (mini-batch GD)\\n- Initialize θ\\n- Repeat until stopping:\\n  - Sample mini-batch B\\n  - g ← (1/|B|) Σ_{i∈B} ∇_θ loss(θ; x_i)\\n  - θ ← θ − η g\\n\\nThat’s the essence: compute the gradient, take a step opposite it, repeat, and tune learning rate and other settings to make it practical and efficient.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1249, 'prompt_tokens': 9, 'total_tokens': 1258, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 512, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'openai/gpt-5-mini', 'system_fingerprint': None, 'id': 'gen-1764062206-9BoyHva0OhsTcKUVS4AC', 'headers': {'date': 'Tue, 25 Nov 2025 09:16:47 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'content-encoding': 'gzip', 'access-control-allow-origin': '*', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self \"https://checkout.stripe.com\" \"https://connect-js.stripe.com\" \"https://js.stripe.com\" \"https://*.js.stripe.com\" \"https://hooks.stripe.com\")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9a400c92dd3ece59-SIN'}, 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--6b6642be-de25-4410-8462-e6d46f5aba91-0', usage_metadata={'input_tokens': 9, 'output_tokens': 1249, 'total_tokens': 1258, 'input_token_details': {}, 'output_token_details': {'reasoning': 512}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc22d635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " 'Short definition\\n- Gradient descent is an iterative optimization algorithm that finds (local) minima of a differentiable function by moving parameters in the direction opposite to the gradient.\\n\\nIntuition\\n- The gradient at a point points in the direction of steepest increase. Moving a small step opposite the gradient decreases the function value most quickly locally. Repeating these steps moves you toward a minimum.\\n\\nMathematics (core update)\\n- Given a loss/objective J(θ) and learning rate η > 0:\\n  θ ← θ − η ∇J(θ)\\n- ∇J(θ) is the vector of partial derivatives (the gradient). The negative sign makes the update go downhill.\\n\\nSimple 1D example\\n- Let f(x) = (x − 3)^2. Then f′(x) = 2(x − 3).\\n- Update: x ← x − η·2(x − 3).\\n- If x0 = 0 and η = 0.1, x1 = 0 + 0.6 = 0.6. Repeating moves x toward 3.\\n\\nVariants\\n- Batch gradient descent: use the full dataset to compute ∇J (stable but expensive for large data).\\n- Stochastic gradient descent (SGD): use the gradient from one training example at a time (noisy, faster per update, useful for large datasets).\\n- Mini-batch gradient descent: use a small batch of examples (common in practice).\\n- Momentum: adds a velocity term to smooth updates and accelerate convergence.\\n- Adaptive methods (AdaGrad, RMSProp, Adam, etc.): adjust per-parameter learning rates based on past gradients.\\n\\nKey hyperparameters and effects\\n- Learning rate (η): too large ⇒ divergence or oscillation; too small ⇒ very slow convergence. Typical practice: tune η, use schedules (decay) or adaptive optimizers.\\n- Batch size: small batches add noise (can help escape shallow minima/saddle points), large batches give stable gradients but higher cost per update.\\n- Momentum and decay parameters: speed convergence and stabilize updates.\\n\\nConvergence and behavior\\n- For convex, smooth functions gradient descent converges to the global minimum (with appropriate step sizes).\\n- For nonconvex problems (e.g., deep neural networks) it converges to a local minimum or saddle point; SGD’s noise often helps find good solutions.\\n- For a quadratic f(x) = 1/2 a x^2, gradient descent converges if 0 < η < 2/a.\\n\\nStopping criteria\\n- Fixed number of iterations/epochs.\\n- Gradient norm small: ||∇J(θ)|| < tolerance.\\n- Change in loss or parameters below threshold.\\n- Validation performance stops improving.\\n\\nPractical tips\\n- Scale/normalize features (helps gradient conditioning).\\n- Initialize parameters sensibly (random small values, or methods like Xavier/He for networks).\\n- Monitor training and validation loss to detect divergence/overfitting.\\n- Use mini-batches and an adaptive optimizer (Adam) as reasonable defaults for many ML problems.\\n- Consider gradient clipping when gradients can explode (RNNs).\\n\\nPseudocode (mini-batch GD)\\n- Initialize θ\\n- Repeat until stopping:\\n  - Sample mini-batch B\\n  - g ← (1/|B|) Σ_{i∈B} ∇_θ loss(θ; x_i)\\n  - θ ← θ − η g\\n\\nThat’s the essence: compute the gradient, take a step opposite it, repeat, and tune learning rate and other settings to make it practical and efficient.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_playground.llm.openrouter import parse_invoke\n",
    "\n",
    "parse_invoke(response, include_reasoning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c60ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OpenRouter LLM client initialization and configuration.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = \"openai/gpt-5-mini\"\n",
    "temperature = 0.0\n",
    "\n",
    "\n",
    "class Schema(BaseModel):\n",
    "    answer: str = Field(description=\"The answer to the question\")\n",
    "    reason: str = Field(description=\"The reason for the answer\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=model,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=temperature,\n",
    "    # reasoning_effort=\"medium\",\n",
    "    reasoning={\n",
    "        \"effort\": \"medium\",  # Can be \"low\", \"medium\", or \"high\"\n",
    "        \"summary\": \"auto\",  # Can be \"auto\", \"concise\", or \"detailed\"\n",
    "    },\n",
    ").with_structured_output(Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7adeeceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 17:27:11,591 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Schema(answer='I can’t tell the weather from this image. It’s a screenshot of code (a message dictionary showing a text and image entry), not a photo of an outdoor scene.', reason='The image contains a code snippet and no visual cues (sky, clouds, sun, precipitation, etc.) that would indicate weather. If you upload or link to a photo of the environment, I can describe the weather from that image.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_playground.llm.multimodal import MediaMessage\n",
    "\n",
    "message = MediaMessage(\"input.png\", description=\"Describe the weather in this image\")\n",
    "ai_msg = llm.invoke([message])\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: As of November 25, 2025, GOOGL (Alphabet Inc.) is trading at $318.58 USD per share. Latest trade time in the feed: 2025-11-25 01:15:00 UTC. Intraday high / low: $327.18 / $305.06. Today's open: $310.81. Change vs previous close: +$19.01 (≈+6.34%). Intraday volume: 85,165,123. Market cap: ~$2.94T. citeturn0finance0\n",
      "Reason: I fetched the live market quote for GOOGL from a financial data feed (finance lookup) and reported the price and key intraday stats along with the feed's timestamp. If you want the price in a specific U.S. timezone, a live chart, or an updated quote, I can fetch that now.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Schema(answer=\"As of November 25, 2025, GOOGL (Alphabet Inc.) is trading at $318.58 USD per share. Latest trade time in the feed: 2025-11-25 01:15:00 UTC. Intraday high / low: $327.18 / $305.06. Today's open: $310.81. Change vs previous close: +$19.01 (≈+6.34%). Intraday volume: 85,165,123. Market cap: ~$2.94T. \\ue200cite\\ue202turn0finance0\\ue201\", reason=\"I fetched the live market quote for GOOGL from a financial data feed (finance lookup) and reported the price and key intraday stats along with the feed's timestamp. If you want the price in a specific U.S. timezone, a live chart, or an updated quote, I can fetch that now.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With structured output, response is already a Pydantic model\n",
    "response: Schema = llm.invoke(\"What is the price of GOOGL today 25-Nov-2025\")\n",
    "print(f\"Answer: {response.answer}\")\n",
    "print(f\"Reason: {response.reason}\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65699461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 16:59:32,576 - WARNING - USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "from langchain_playground.tools.youtube import youtube_loader\n",
    "\n",
    "\n",
    "@tool\n",
    "def scrape_youtube_video(youtube_url: str) -> str:\n",
    "    \"\"\"Scrape a YouTube video and return its transcript and metadata.\n",
    "\n",
    "    This tool extracts the transcript, title, channel, duration, views, likes,\n",
    "    and other metadata from a YouTube video URL.\n",
    "\n",
    "    Args:\n",
    "        youtube_url: The YouTube video URL to scrape (e.g., https://www.youtube.com/watch?v=VIDEO_ID)\n",
    "\n",
    "    Returns:\n",
    "        A formatted string containing video metadata and transcript\n",
    "    \"\"\"\n",
    "    return youtube_loader(youtube_url)\n",
    "\n",
    "\n",
    "tools = [scrape_youtube_video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create a model for the agent (without structured output, as agents handle tool calling)\n",
    "agent_llm = ChatOpenAI(\n",
    "    model=model,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=temperature,\n",
    "    reasoning_effort=\"medium\",\n",
    "    extra_body={\"plugins\": [{\"id\": \"web\"}]},\n",
    ")\n",
    "\n",
    "# Create a simple agent with the YouTube scraper tool\n",
    "youtube_agent = create_agent(\n",
    "    model=agent_llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful assistant that can analyze YouTube videos. When given a YouTube URL, use the scrape_youtube_video tool to get the video transcript and metadata, then provide insights or answer questions about the video content.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 16:59:34,697 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 16:59:42,210 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I scraped the video and its transcript. Short summary first, then the main points.\n",
      "\n",
      "Video metadata (from the scrape)\n",
      "- Title: Rick Astley - Never Gonna Give You Up (Official Video) (4K Remaster)\n",
      "- Channel: Rick Astley\n",
      "- Duration: 3:34\n",
      "- Published: Oct 24, 2009\n",
      "- Views/likes (at scrape): ~1.72B views, ~18.6M likes\n",
      "\n",
      "Summary\n",
      "- The transcript is the lyrics to the song “Never Gonna Give You Up.” The central message is a repeated, emphatic promise of loyalty, commitment, and reassurance from the singer to a romantic partner.\n",
      "\n",
      "Main points/themes in the lyrics\n",
      "- Promise of unwavering commitment: “A full commitment’s what I’m thinking of,” and repeated lines “Never gonna give you up / Never gonna let you down.”\n",
      "- Reassurance against hurt or abandonment: vows not to run around, desert, make cry, say goodbye, tell lies or hurt the partner.\n",
      "- Shared history and mutual understanding: “We’ve known each other for so long,” implying familiarity and emotional closeness.\n",
      "- Acknowledgement of emotional pain/shyness: “Your heart’s been aching but you’re too shy to say it.”\n",
      "- Playful recognition of romantic “rules” and the “game” of love: lines like “You know the rules and so do I” and “We know the game and we’re gonna play it.”\n",
      "- Repetition for emphasis: the chorus is repeated many times to reinforce the promise and the song’s upbeat, reassuring tone.\n",
      "\n",
      "If you want, I can:\n",
      "- Provide a verse-by-verse breakdown,\n",
      "- Extract notable lines or timestamps from the video,\n",
      "- Or generate a short recap suitable for sharing (tweet/description). Which would you prefer?\n"
     ]
    }
   ],
   "source": [
    "# Example: Scrape a YouTube video and ask questions about it\n",
    "# Replace with your own YouTube URL\n",
    "youtube_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"  # Example URL\n",
    "\n",
    "response = youtube_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": f\"Please scrape this YouTube video: {youtube_url} and summarize the main points discussed in the transcript.\"}]})\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
